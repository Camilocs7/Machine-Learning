# Predicci√≥n de Series Temporales con LSTM + Atenci√≥n
Microservicio anal√≠tico en Python/TensorFlow que entrena modelos **LSTM** y **BiLSTM** con un mecanismo de **Atenci√≥n** personalizado para predecir precios diarios por producto. Proyecto aplicado de Deep Learning para predicci√≥n de series temporales.

## Caracteristicas principales
- **Modelos:** Implementaci√≥n de LSTM cl√°sico, LSTM Bidireccional y una Capa de Atenci√≥n personalizada (TensorFlow/Keras).
- **Entrenamiento:** Secuencial por ID de producto con normalizaci√≥n MinMax, ventanas temporales y separaci√≥n train/val/test.
- **Optimizaci√≥n:** **EarlyStopping** y guardado autom√°tico del mejor modelo (en formato `.h5`) por cada producto, basado en la m√©trica de validaci√≥n.
- **M√©tricas:** C√°lculo y reporte de **MAE** (Mean Absolute Error) y **RMSE** (Root Mean Squared Error) por serie temporal.
- **Reproducibilidad:** Entorno virtual (`venv`) + `requirements.txt`; opci√≥n de **Docker** y **Docker Compose** con JupyterLab para un entorno de desarrollo integrado y reproducible.
- **An√°lisis:** Notebook principal (`codigo_completo_LSTM_ATTENTION.ipynb`) para an√°lisis, visualizaci√≥n y experimentos detallados.


## Instalacion y ejecucion

### Opci√≥n A ‚Äî Ejecutar SIN Docker (entorno local)
1) **Clonar el repositorio:**
    ```bash
    git clone git@github.com:Camilocs7/Machine-Learning.git
    cd Machine-Learning
    ```
2) **Crear y activar entorno virtual:**
    ```bash
    # Linux/macOS
    python3 -m venv venv
    source venv/bin/activate
    # Windows (ejecutar en PowerShell)
    python -m venv venv
    .\venv\Scripts\activate
    ```
3) **Instalar dependencias:**
    ```bash
    pip install --upgrade pip
    pip install -r requirements.txt
    ```
4) **Verificar TensorFlow y Ejecutar entrenamiento:**
    ```bash
    # Verifica la instalaci√≥n (opcional)
    python -c "import tensorflow as tf; print(tf.__version__)"
    # Inicia el loop de entrenamiento por producto
    python src/entrenar_lstm_attention_loop.py
    ```

### Opci√≥n B ‚Äî Ejecutar CON Docker (recomendado)
1) **Construir la imagen:**
    ```bash
    docker build -t lstm-attention .
    ```
2) **Levantar el entorno con JupyterLab:**
    ```bash
    docker-compose up
    ```
3) **Acceder y ejecutar:**
    - Abrir el navegador en `http://localhost:8888` (acceso directo, sin token).
    - Abrir el notebook `codigo_completo_LSTM_ATTENTION.ipynb` y ejecutar las celdas.

---

## Datos de entrada esperados
Los archivos de datos **no se incluyen** en el repositorio (ignorados por `.gitignore`) debido a su tama√±o.

- **Fuente:** Se pueden descargar desde la fuente en Kaggle:
  üëâ [https://www.kaggle.com/datasets/paulohernandezt/price-product-chile/data](https://www.kaggle.com/datasets/paulohernandezt/price-product-chile/data)
- **Archivos Esperados en `data/`:**
  - `precio_2023.csv`, `precio_2024.csv`, `precio_2025.csv`
  - `precios_combinados2.csv`, `precios_estandarizados*.csv`, `precios_final_features2.csv`
- **Preprocesamiento:** Los datos son autom√°ticamente unidos, limpiados y normalizados (`MinMaxScaler`) en el *pipeline* antes de ser usados en la creaci√≥n de las ventanas temporales para el entrenamiento.

## Arquitectura del Modelo (src/attention_layer.py)


El modelo de predicci√≥n de series temporales utiliza una arquitectura secuencial con un mecanismo de atenci√≥n para mejorar la precisi√≥n y la interpretabilidad.

**Componentes principales:**
1.  **Capa de entrada:** Formato de ventana temporal.
2.  **LSTM o BiLSTM:** Extrae caracter√≠sticas secuenciales de los datos.
3.  **AttentionLayer personalizada:**
    * Calcula din√°micamente **pesos** por cada *timestamp* (paso de tiempo) de la secuencia.
    * Genera un vector de **contexto ponderado** que enfatiza la informaci√≥n relevante para la predicci√≥n.
4.  **Capa Dense final:** Capa de regresi√≥n para generar el valor de predicci√≥n.

**Diagrama simplificado:**

Input (Secuencia) ‚Üí LSTM/BiLSTM ‚Üí AttentionLayer ‚Üí Dense ‚Üí Output (Predicci√≥n)


## Ejecuci√≥n del Pipeline Completo
El script `entrenar_lstm_attention_loop.py` orquesta el proceso:

1.  Carga y prepara los datos de entrada.
2.  Itera sobre la lista de IDs de productos √∫nicos.
3.  Para cada `product_id`:
    * Genera las ventanas temporales.
    * Construye y compila el modelo (LSTM + Atenci√≥n).
    * Entrena, aplicando *EarlyStopping*.
    * Eval√∫a el modelo y registra las m√©tricas.
    * Guarda el modelo √≥ptimo en `lstm_prueba3/models/prod_<ID>/best_model.h5`.

## Resultados y Rendimiento
- **Salida:** Un modelo √≥ptimo (`best_model.h5`) y las m√©tricas **MAE** y **RMSE** se generan por cada serie temporal procesada.
- **Rendimiento:** El uso de la Capa de Atenci√≥n mejora la capacidad del modelo para identificar patrones cruciales, especialmente en series con alta **variabilidad** o **ruido**, llevando a predicciones m√°s precisas que un LSTM sin atenci√≥n.
- **Reproducibilidad:** Los resultados son completamente reproducibles utilizando el entorno Docker.

## Autor
Proyecto elaborado por Camilo Cerda Sarabia.
*Proyecto aplicado a predicci√≥n de series temporales ‚Äî 2025.*
